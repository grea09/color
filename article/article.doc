<h1 id="introduction" class="unnumbered">Introduction</h1>
<p>Automated planning aims to synthesize a set of actions into an ordered plan to achieve the user's goals. Most academic research on general purpose planning was, until the end of the 90s, oriented toward plan space planning. It was praised for its least commitment orientation that makes it more efficient than classical planning and also more of an elegant solution for its flexibility. However, more recently, drastic improvements in state search planning was made possible by advanced and efficient heuristics. This allowed those planners to scale up more easily than plan-space search ones, notably thanks to approaches like GraphPlan <span class="citation">(Blum and Furst 1997)</span>, fast-forward <span class="citation">(Hoffmann 2001)</span> and LAMA <span class="citation">(Richter, Westphal, and Helmert 2011)</span>.</p>
<p>This search of scalability and performance shadows the greatest advantage of Partial-Order Planning (POP): flexibility. This advantage allows POP to efficiently build plans with the parallel use of actions. <!-- Nice idea about controlled loops with Mathis 2014-02-18 --> It also means that it can refine broken plans and optimize them further than a regular state-based planner. <!-- FIXME speculation --> This was shown clearly during the golden age of POP with UCPOP <span class="citation">(Penberthy, Weld, and others 1992)</span> <!-- No, it wasn't shown !-->. It was a reference in terms of quality and expressiveness for years, to such extends that other works tried to inject state-space planners with its advances <span class="citation">(Gazen and Knoblock 1997)</span>.</p>
<p>In this paper, we explore new ideas to revive POP as an attractive alternative to other totally ordered approach. Some papers like the appropriately named &quot;Reviving partial order planning&quot; <span class="citation">(Nguyen and Kambhampati 2001)</span> and VHPOP <span class="citation">(Younes and Simmons 2003)</span> already tried to extend POP into such an interesting alternative. More recent efforts <span class="citation">(A. J. Coles et al. 2010; Sapena, Onaindia, and Torreno 2014)</span> are trying to adapt the powerful heuristics from state-based planning to POP's approach.</p>
<p>Our approach is different: we want to build a robust and quality oriented planner without losing speed. <!-- we'll try at least --> In order to achieve this, the planner is implemented in its recursive form and takes a partial plan as an input. Our system prepopulates this plan with a proper domain plan that was processed during the domain compilation phase. This allows it to have a significant head-start in relation to most planners as the plan is already almost complete in most cases. This is described in section 2 of the present paper.</p>
<p>The problem with this is that this partial plan contains problems that can break the POP algorithm (such as cycles) or that can drive it toward nonoptimal results. This is why we focus the section 3 of our paper on plan quality and negative refinements. This leads naturally toward the introduction of negative flaws that aims to optimise the plan: the cycle, the alternative and the orphan flaws.</p>
<p>This causes another problem since POP wasn't made to select these flaws as they can interfere with positive flaws by deconstructing their work. That is the reason behind the section 4 of our work: goal and flaw selection that aims to reduce the branching factor of our algorithm. This will allow greater speed and better plan quality.</p>
<p>This leads to a complete and coherent planning algorithm that gives optimised plans rather efficiently. The algorithm with its phases and properties is explained in the section 5 with the experimental result presented in detail in section 6.</p>
<p>In order to present our aLternative Optimization with partiaL pLan Injection Partial Ordered Planning (LOLLIPOP) system, we need to explain the classical POP framework and its limits.</p>
<h1 id="classical-partial-order-planning-framework">Classical Partial Order Planning Framework</h1>
<p>In this paper, we decided to build our own planning framework based on PDDL's concepts. This new framework is called WORLD as it is inspired by more generalistic world description tools such as RDF Turtle <span class="citation">(W3C 2014)</span>. It is about equivalent in expressiveness to PDDL 3.1 with object-fluents support <span class="citation">(Kovacs 2011)</span>.</p>
<p>We chose this type of domain description because we plan to extend on the work of Göbelbecker et al. <span class="citation">(Göbelbecker et al. 2010)</span> in order to make this planner able to do soft resolution in future works. The next definitions are based on the ones exposed in this paper.</p>
<h2 id="domain">Domain</h2>
<p>Every planning paradigm needs a way to represent its fluents and operators. Our planner is based on a rather classical domain definition with lifted operators and representing the fluents as propositional statements</p>
<p>We define our <strong>planning domain</strong> as a tuple <span class="math inline"><em>Δ</em> = ⟨<em>T</em>, <em>C</em>, <em>P</em>, <em>F</em>, <em>O</em>⟩</span> where</p>
<ul>
<li><span class="math inline"><em>T</em></span> are the <strong>types</strong>,</li>
<li><span class="math inline"><em>C</em></span> is the set of <strong>domain constants</strong>,</li>
<li><span class="math inline"><em>P</em></span> is the set of <strong>properties</strong> with their arities and typing signatures,</li>
<li><span class="math inline"><em>F</em></span> represents the set of <strong>fluents</strong> defined as potential equations over the terms of the domain,</li>
<li><span class="math inline"><em>O</em></span> is the set of optionally parameterized <strong>operators</strong> with preconditions and effects.</li>
</ul>
<p>The symbol system is completed with a notion of <strong>term</strong> (either a constant, a variable parameter or a property) and a few relations. We provide types with a relation of <strong>subsumption</strong> noted <span class="math inline"><em>t</em><sub>1</sub> ≺ <em>t</em><sub>2</sub></span> with <span class="math inline"><em>t</em><sub>1</sub>, <em>t</em><sub>2</sub> ∈ <em>T</em></span> meaning that all instances of <span class="math inline"><em>t</em><sub>1</sub></span> are also instances of <span class="math inline"><em>t</em><sub>2</sub></span>. On terms, we add two relations: the <strong>assignation</strong> (noted <span class="math inline">←</span>) and the <strong>potential equality</strong> (noted <span class="math inline">≐</span>).</p>
<h2 id="problem">Problem</h2>
<p>Along with a domain, every planner needs a problem description in order to work. For this, we use a simplified version of the classical problem representation with some special additions.</p>
<p>From there we add the definition of a planning problem as the tuple <span class="math inline"><em>Π</em> = ⟨<em>Δ</em>, <em>C</em><sub><em>Π</em></sub>, <em>I</em>, <em>G</em>, <em>p</em>⟩</span> where</p>
<ul>
<li><span class="math inline"><em>Δ</em></span> is a planning domain,</li>
<li><span class="math inline"><em>C</em><sub><em>Π</em></sub></span> is the set of <strong>problem constant</strong> disjoint from <span class="math inline"><em>C</em></span>,</li>
<li><span class="math inline"><em>I</em></span> is the <strong>initial state</strong>,</li>
<li><span class="math inline"><em>G</em></span> is the <strong>goal</strong>,</li>
<li><span class="math inline"><em>p</em></span> is a given <strong>partial plan</strong>.</li>
</ul>
<p>The framework uses the <em>closed world assumption</em> in wich all undefined predicates are false in the initial state and undefined properties doesn't have a value.</p>
<p>Even if the present framework is based upon classical plan space planning it introduces some key differences. For example, we need to add the partial plan as a problem parameter since our approach requires it. We define a partial plan as a tuple <span class="math inline">⟨<em>S</em>, <em>L</em>, <em>B</em>⟩</span> with <span class="math inline"><em>S</em></span> the set of <strong>steps</strong> (instantiated operators also called actions), <span class="math inline"><em>L</em></span> the set of <strong>causal links</strong>, and <span class="math inline"><em>B</em></span> the set of <strong>binding constraints</strong>. In classic representations, we also add ordering constraints that were voluntarily omitted here. Since the causal links always are supported by an ordering constraint and since the only case where bare ordering constraints are needed is in threats we decided to represent them with &quot;bare causal links&quot;. These are stored as causal links without bearing any fluents. The old ordering constraint can still be achieved using the transitive form of the causal links. That allows us to introduce the <strong>precedence operator</strong> noted <span class="math inline"><em>a</em><sub><em>i</em></sub> ≻ <em>a</em><sub><em>j</em></sub></span> iff there is a path of causal links that connects <span class="math inline"><em>a</em><sub><em>i</em></sub></span> with <span class="math inline"><em>a</em><sub><em>j</em></sub></span> with <span class="math inline"><em>a</em><sub><em>i</em></sub></span> being anterior to <span class="math inline"><em>a</em><sub><em>j</em></sub></span>.</p>
<h2 id="flaws">Flaws</h2>
<p>A specificity of Partial Order Planning is that it must fix flaws in a partial plan in order to refine it into a valid plan that is a solution to the given problem. In this section, we define the classical flaws.</p>
<div class="definition" name="Subgoal">
<dt>Definition 1</dt>
<dd>
<p>What we call subgoal is the type of flaw that consists into a missing causal link to satisfy a precondition of a step. We can note a subgoal as: <br /><span class="math display">$$a_p \xrightarrow{s} a_n \notin L \mid \{ a_p, a_n \} \subseteq S $$</span><br /> with <span class="math inline"><em>a</em><sub><em>n</em></sub></span> called the <strong>needer</strong> and <span class="math inline"><em>a</em><sub><em>p</em></sub></span> an eventual <strong>provider</strong> of the fluent <span class="math inline"><em>s</em></span>. This fluent is called <strong>proper fluent</strong> of the subgoal.</p>
</dd>
</dl>
</div>
<div class="definition" name="Threat">
<dt>Definition 1</dt>
<dd>
<p>We call a threat a type of flaws that consist of having an effect of a step that can be inserted between two actions with a causal link that is intolerant to said effect. We can say that a step <span class="math inline"><em>a</em><sub><em>b</em></sub></span> is said to threaten a causal link <span class="math inline">$a_p \xrightarrow{t} a_n$</span> iff <br /><span class="math display">¬<em>t</em> ∈ <em>e</em><em>f</em><em>f</em>(<em>a</em><sub><em>b</em></sub>)∧<em>a</em><sub><em>p</em></sub> ≻ <em>a</em><sub><em>b</em></sub> ≻ <em>a</em><sub><em>n</em></sub> ⊨ <em>L</em></span><br /> In this case we call the action <span class="math inline"><em>a</em><sub><em>b</em></sub></span> the <strong>breaker</strong>, <span class="math inline"><em>a</em><sub><em>n</em></sub></span> the needer and <span class="math inline"><em>a</em><sub><em>p</em></sub></span> provider of the proper fluent <span class="math inline"><em>t</em></span>.</p>
</dd>
</dl>
</div>
<h2 id="resolvers">Resolvers</h2>
<p>These flaws are fixed via the application of a resolver to the plan. A flaw can have several resolvers that match its needs.</p>
<div id="def:resolver" class="definition" name="Resolvers">
<dt>Definition 2</dt>
<dd>
<p>A resolver is a potential causal link. We can note it as a tuple <span class="math inline"><em>r</em> = ⟨<em>a</em><sub><em>s</em></sub>, <em>a</em><sub><em>t</em></sub>, <em>f</em>⟩</span> with :</p>
<ul>
<li><span class="math inline"><em>a</em><sub><em>s</em></sub></span> and <span class="math inline"><em>a</em><sub><em>t</em></sub></span> being the source and target of the resolver,</li>
<li><span class="math inline"><em>f</em></span> being the considered fluent.</li>
</ul>
</dd>
</dl>
</div>
<p>For standard flaws, the resolvers are simple to find. For a <em>subgoal</em> the resolvers are a set of the potential causal links between a possible provider of the proper fluent and the needer. To solve a <em>threat</em> there is mainly two resolvers: a causal link between the needer and the breaker called <strong>demotion</strong> or a causal link between the breaker and the provider called <strong>promotion</strong>.</p>
<h2 id="related-flaws">Related Flaws</h2>
<p>Once the resolver is applied, another important step is needed in order to be able to keep refining. The algorithm needs to find the related flaws of the elected resolver. These related flaws are searched by type.</p>
<p>The algorithm for searching related flaws is a function that needs the <em>trouble maker</em> and the problem. A <strong>trouble maker</strong> is the <em>source</em> of the resolver if it was inserted into the plan. There is no need to search for related flaws when fixing a threat or when simply adding a causal link.</p>
<h2 id="algorithm">Algorithm</h2>
<p>The classical POP algorithm is pretty straight forward: it starts with a simple partial plan and refines its <em>flaws</em> until they are all resolved to make the found plan a solution of the problem.</p>
<div id="alg:pop" class="algorithm" name="Classical Partial Order Planning">
<dt>Algorithm 1</dt>
<dd>
    Success   Flaw <span class="math inline"><em>f</em>←</span>   Resolvers <span class="math inline"><em>R</em>←</span>      <span class="math inline"><em>a</em> ← <em>a</em>∪</span>     Sucess     Failure  
</dd>
</dl>
</div>
<p>The algorithm 1 was inspired by <span class="citation">(Ghallab, Nau, and Traverso 2004)</span>. This POP implementation uses an agenda of flaws that is efficiently updated after each refinement of the plan. At each recursion, it selects a flaw in the agenda to remove it for processing. It then selects a resolver and tries to apply it. If it fails to apply all resolvers the algorithm backtracks to last choice to try another one. The algorithm terminates when no more resolvers fit a flaw or when all flaws have been fixed.</p>
<h2 id="limitations">Limitations</h2>
<p>This standard implementation has several limitations. First it can easily make poor choices that will lead to excessive backtracking. It also can't undo redundant or nonoptimal links if they don't fail. The last part is that if we input a partial plan that has problems into the algorithm it can break and either not returning a solution to a solvable problem or give an invalid solution.</p>
<div class="figure">
<img src="graphics/example.png" alt="Figure 1: Extract of example domain and problem" />
<p class="caption">Figure 1: Extract of example domain and problem</p>
</div>
<p>To illustrate these limitations, we use the example described in figure 1 where a robot must fetch a lollipop in a locked room. This problem is quite easily solved by regular POP algorithms. However, we can observe that if we inserted an operator that has a similar effect as <span class="math inline"><em>g</em><em>o</em></span> but that has impossible precondition (e.g. <span class="math inline"><em>f</em><em>a</em><em>l</em><em>s</em><em>e</em></span>), then the algorithm might select it and will need backtracking to solve the plan. This problem is solved via using simple goal selection methods. However, to our knowledge, this was never applied in the context of POP.</p>
<p>Another limitation is met when the input plan contains a loop or a contradiction. We consider a partial plan similar to <span class="math inline"><em>I</em> → <em>g</em><em>o</em>(<em>r</em><em>o</em><em>b</em><em>o</em><em>t</em>, <em>l</em><em>i</em><em>v</em><em>i</em><em>n</em><em>g</em><em>r</em><em>o</em><em>o</em><em>m</em>)→<em>g</em><em>r</em><em>a</em><em>b</em>(<em>r</em><em>o</em><em>b</em><em>o</em><em>t</em>, <em>k</em><em>e</em><em>y</em><em>s</em>)→<em>g</em><em>o</em>(<em>r</em><em>o</em><em>b</em><em>o</em><em>t</em>, <em>c</em><em>o</em><em>r</em><em>r</em><em>i</em><em>d</em><em>o</em><em>r</em>)→<em>g</em><em>o</em>(<em>r</em><em>o</em><em>b</em><em>o</em><em>t</em>, <em>l</em><em>i</em><em>v</em><em>i</em><em>n</em><em>g</em><em>r</em><em>o</em><em>o</em><em>m</em>)…</span>. There is a loop in that plan (between the two <span class="math inline"><em>g</em><em>o</em></span> actions) that standard POP algorithms won't fix. In the literature this is not considered since classical POP doesn't take a partial plan as input or hypothesise out directly such inconsistencies. <!-- FIXME Not flagrant in current example, others could argue it is all artificial and that the problem doesn't really exists--></p>
<h1 id="sec:properplan">Proper Plan Generation and Injection</h1>
<p>One of the main contributions of the present paper is our use of what we call a <em>domain proper plan</em> in order to quickly derive a partial plan from it.</p>
<h2 id="definition">Definition</h2>
First of all we need to define what is a domain proper plan.
<div class="definition" name="Domain proper plan">
<dt>Definition 2</dt>
<dd>
<p>The proper plan <span class="math inline"><em>Δ</em><sup><em>P</em></sup></span> of a planning dommain <span class="math inline"><em>Δ</em></span> is a labelled directed graph that binds two operators <span class="math inline">$o_1 \xrightarrow{f} o_2$</span> iff it exists at least an unifying fluent <span class="math inline"><em>f</em> ∈ <em>e</em><em>f</em><em>f</em>(<em>o</em><sub>1</sub>)∩<em>p</em><em>r</em><em>e</em>(<em>o</em><sub>2</sub>)</span> between them.</p>
</dd>
</dl>
</div>
<p>This definition was inspired by the notion of domain causal graph as explained in <span class="citation">(Göbelbecker et al. 2010)</span> and originally used as heuristic in <span class="citation">(Helmert 2006)</span>. A variation of this notion was used in <span class="citation">(Peot and Smith 1994)</span> that directly binds the goal and uses precondition nodes instead of labels. That makes the proper plan a kind of operator dependency graph of the domain. With this information, we can know how potentially useful an operator can be in any plan and also where to look for providers of a fluent.</p>
<h2 id="generation">Generation</h2>
<p>This plan is computed during the domain compilation time and, therefore, gives an advantage to the POP algorithm. For more explanation of phases see section 5.</p>
<div id="alg:properplan" class="algorithm" name="Domain proper plan generation algorithm">
<dt>Algorithm 2</dt>
<dd>
   
</dd>
</dl>
</div>
<p><!--TODO Of Course ! https://youtu.be/1W7c8QghPxk --></p>
<p>The algorithm 2 is based upon the previous definition. It will explore the operators space and build a causal map to know wich operator causes what. Once done it will iterate on every precondition and search for a satisfying cause in order to add the causal link to the proper plan.<!--FIXME rewrite that once the algorithm is done --></p>
<p>This proper plan is saved for usage related to heuristics (cf. section 4.1).</p>
<h2 id="initial-and-goal-step-injection">Initial and Goal Step Injection</h2>
<p>The next step is to derive a viable partial plan from the proper plan. The main problem with this is the lack of initial or goal step in it. Since it is made during domain compilation time the algorithm doesn't have access to the problem's data. That is why during the problem processing phase, an algorithm will inject the initial and goal step into the plan. This uses the same algorithm used to build the proper plan in the first place. It will bind the initial step to the operators that can be used in the initial world state and the goal step to the operators that can fulfil its preconditions. We decide to leave the operators uninstantiated for our cycle breaking mechanism to work. This mechanism is based on the alternative negative flaw (cf. definition ??)</p>
<div class="figure">
<img src="graphics/properplan.png" alt="Figure 2: Proper plan of example domain with dotted initial and goal step insertion." />
<p class="caption">Figure 2: Proper plan of example domain with dotted initial and goal step insertion.</p>
</div>
<p>In figure 2 we illustrate the proper plan mechanics with our previous example. The most notable feature of this graph is its coverage. It contains cycles wich can be proven problematic for POP.</p>
<p>However, cycles contain information regarding the dependencies of operators. We call <em>co-dependent</em> several operators that form a cycle. If the cycle is made of only one operator (self-loops) then it is called <em>auto-dependent</em>. This information help detects early inconsistencies in the plan when instantiating the operators in relation to the initial and goal step. The solution is then to simply remove inconsistent causal links while saving them as potentially problematic. Then the initialization algorithm only has to break the remaining loops using cycle flaws. This flaw is described in section 3.</p>
<p>The last of these problems is that even if the proper plan can be coherent and even solve the problem, it may contain many unnecessary steps. This is the main reason why we introduce <em>negative refinements</em> in the next section.</p>
<h1 id="sec:negative">Negative Refinements and Plan Optimization</h1>
<p>The Classical POP algorithm works upon a principle of positive plan refinements. The two standard flaws (subgoals and threats) are fixed by <em>adding</em> steps, causal links, or variable binding constraints to the partial plan. In our case, it is important to be able to <em>remove</em> part of the plan that isn't necessary or even dangerous to the solution.</p>
<h2 id="negative-flaws">Negative Flaws</h2>
<p>Since we are given a partial plan that is quite complete, we need to add new flaws to optimize and fix this plan into the best one possible. These flaws are called <em>negative</em> since their resolvers differ from classical ones from their effects on the plan.</p>
<div class="definition" name="Cycle">
<dt>Definition 2</dt>
<dd>
<p>A cycle is a flaw corresponding to a set of causal links that forms a loop. A causal link <span class="math inline"><em>a</em><sub><em>i</em></sub> → <em>a</em><sub><em>j</em></sub></span> belongs to a cycle iff it exists a path from <span class="math inline"><em>a</em><sub><em>j</em></sub></span> to <span class="math inline"><em>a</em><sub><em>i</em></sub></span>. This path can be of lenght <span class="math inline">0</span> in whitch case <span class="math inline"><em>a</em><sub><em>i</em></sub> = <em>a</em><sub><em>j</em></sub></span> and the cycle is a self-loop.</p>
</dd>
</dl>
</div>
<p>The complete path of a cycle is called a <strong>closed walk</strong>. In order to improve runtime efficiency, the cycles are detected as the proper plan is built: during domain compilation phase (see more about phases section 5).</p>
<div id="def:alternative" class="definition" name="Alternative">
<dt>Definition 3</dt>
<dd>
<p>An alternative is a negative flaw that occurs when it exists a better provider choice for a given link. We can say that an alternative to a causal link <span class="math inline">$a_p \xrightarrow{f} a_n$</span> is a provider <span class="math inline"><em>a</em><sub><em>b</em></sub></span> that have a better <em>utility value</em> than <span class="math inline"><em>a</em><sub><em>p</em></sub></span> (cf. section <strong>??</strong>)</p>
</dd>
</dl>
</div>
<p>The alternatives are expensive to find. For this reason, they are found only once during the initialization phase (cf. section 5).</p>
<div class="definition" name="Orphan">
<dt>Definition 3</dt>
<dd>
<p>An orphan is a negative flaw that means that a step in the partial plan (that is not the initial or goal step) is not participating in the plan. In other words <span class="math inline"><em>a</em><sub><em>o</em></sub></span> is an orphan iff <span class="math inline"><em>a</em><sub><em>o</em></sub> ≠ <em>I</em> ∧ <em>a</em><sub><em>o</em></sub> ≠ <em>G</em> ∧ <em>p</em>.<em>d</em><sup>+</sup>(<em>a</em><sub><em>o</em></sub>)=0</span>.</p>
</dd>
</dl>
</div>
<p>With <span class="math inline"><em>p</em>.<em>d</em><sup>+</sup>(<em>a</em><sub><em>o</em></sub>)</span> being the outgoing degree of <span class="math inline"><em>a</em><sub><em>o</em></sub></span> in the dirrected graph formed by <span class="math inline"><em>p</em></span>.</p>
<h2 id="negative-refinements">Negative Refinements</h2>
<p>With the introduction of negative flaws comes the modification of resolvers to handle negative refinements.</p>
We add onto the definition 3 :
<div class="definition" name="Signed Resolvers">
<dt>Definition 3</dt>
<dd>
<p>A signed resolver is a resolver with a notion of sign. We add to the resolver tuple <span class="math inline"><em>s</em></span> as the sign of the resolver in <span class="math inline">{ + , − }</span>.</p>
</dd>
</dl>
</div>
<p>An alternative notation for the signed resolver is inspired by the causal link notation with simply the sign underneath : <br /><span class="math display">$$r = a_s \xrightarrow[+/-]{f} a_t$$</span><br /></p>
<p>The previously defined negative flaws have all their associated negative resolvers.</p>
<p>A <em>cycle</em> will have as negative resolvers each causal link belonging to its closed walk. This way the algorithm will know that there are no solution if there aren't any way to remove any causal links of the cycle. An improvement upon this would be to sort the links with the utility value of their target. <!-- FIXME actually do this--></p>
<p>The solution to an alternative is a negative refinement that simply remove the targeted causal link. We count on the fact that this will create a new subgoal that will choose the better alternative.</p>
<p>The way to fix an orphan is quite simple. The negative refinement is only meant to remove the targeted action and its incoming causal link while tagging the sources of them as potential orphans.</p>
<h2 id="related-flaws-for-negative-resolvers">Related Flaws For Negative Resolvers</h2>
<p>The standard mechanism of related flaws needs an upgrade since the new kind of flaws can easily cause side effects and even interfere with one another. All of this is explained in the figure 4. This figure contains the notion of <strong>side effects</strong> of flaws. This notion is important to understand how to properly search for related flaws. The first thing to notice in the system is that when treating positive resolvers nothing need to change from the classical method. When dealing with negative resolver we need to search for additional subgoals and threats. In fact negative refinements will most likely cause an increase in subgoals or threats. Another side effects of negative refinements is on the generation of possible orphan flaws.</p>
<h1 id="sec:selection">Driving Flaw and Resolver Selection</h1>
<p>Resolver and flaws selection are the keys to improvements in performances. Choosing a good resolver helps to reduce the branching factor that accounts for most of the time spent on running POP algorithms.</p>
<h2 id="sec:heuristics">Operator Oriented Heuristics</h2>
<p>A heuristic is a function that allows to rank operators. This is at the heart of the algorithm since it will try to make the best choice for goal selection. Driving choice was already shown as a performance improvement mechanism in <span class="citation">(Younes and Simmons 2003)</span> as several heuristics were combined to improve POP's efficiency.</p>
<p>In our case, we chose to have one main heuristic that aims to lower branching factor by trying to make the base operations more aware of the utility of the considered data. A good explanation of the mechanics behind this can be found in <span class="citation">(Kambhampati 1994)</span>.</p>
<p>The aim is to create heuristics that have a sense of the usefulness of an operator or step. In order to do that, we need to create a function that will give a higher value the more the action participates potentially or effectively in a plan and a lower value the more it is needy.</p>
<p>We tried different function that could accomplish this and compared them together. Since we rely heavily on the notion of positive and negative degree, we need to define them first:</p>
<div class="definition" name="Degrees">
<dt>Definition 3</dt>
<dd>
<p>Degrees are meant to measure the usefulness of an operator. The notion is derived from the incoming and outgoing degree of a node in a graph.</p>
<p>We note <span class="math inline"><em>g</em>.<em>d</em><sup>+</sup>(<em>o</em>)</span> and <span class="math inline"><em>g</em>.<em>d</em><sup>−</sup>(<em>o</em>)</span> respectively the outgoing and incoming degree of an operator in a plan. These represent the number of causal links that goes out or toward the operator. We call proper degree of an operator <span class="math inline"><em>o</em>.<em>d</em><sup>+</sup> = |<em>e</em><em>f</em><em>f</em>(<em>o</em>)|</span> and <span class="math inline"><em>o</em>.<em>d</em><sup>−</sup> = |<em>p</em><em>r</em><em>e</em>(<em>o</em>)|</span> the number of potential usefulness of an operator based on its number of preconditions and effects.</p>
</dd>
</dl>
</div>
<p>There are several ways to use the degrees as an indicator. The goal is to increase the <em>utility value</em> with every <span class="math inline"><em>d</em><sup>+</sup></span>, since this reflects a positive participation in the plan, and decrease it with every <span class="math inline"><em>d</em><sup>−</sup></span> since actions with a higher incoming degree means being harder to satisfy. With this idea in mind, we chose to try several approaches to compute the utility value. In order to unify the notation we decide to transform the data into tuples with degrees ordered from the most specific to the most general : <span class="math inline"><em>d</em><sup>±</sup>(<em>o</em>)=⟨<em>P</em>.<em>d</em><sup>±</sup>(<em>o</em>),<em>Δ</em>.<em>d</em><sup>±</sup>(<em>o</em>),<em>o</em>.<em>d</em><sup>±</sup>⟩</span></p>
<h3 id="simple-degree-product">Simple Degree Product</h3>
<p>One of the first ways that come to mind in order to achieve this is to simply subtract the negative degrees from the positive ones and to do the product of all the results:</p>
<p><br /><span class="math display">$$
h_{simple} = \alpha_{simple} \prod_{i=1}^3 d^+_i(o) - d^-_i(o)
$$</span><br /></p>
<p>with <span class="math inline"><em>α</em><sub><em>s</em><em>i</em><em>m</em><em>p</em><em>l</em><em>e</em></sub></span> being the unification constant. It is used to adjust the value of the heuristics to similar levels as to other heuristics for comparison. We can see two problems with this approach: it gets to zero if any data misses or if any degree cancels each other. This is problematic since it means that the result gets less entropy from the data than it should.</p>
<h3 id="simple-ratio">Simple Ratio</h3>
<p>Another way to do this is to do a ration of the degrees. The problem is that it often occurs that any divisor will turn out to be zero, wich is problematic. A simple workaround is to simply add a constant to the divisor :</p>
<p><br /><span class="math display">$$
h_{ratio} = \prod_{i=1}^3 {{d^+_i(o)} \over {d^-_i(o) + \alpha_{ratio}}}
$$</span><br /></p>
<h3 id="logarithmic-ratio">Logarithmic Ratio</h3>
<p>Taking the previous approach and trying another way to deal with null divisor gave this solution. We use powers to actually transform the quotient into a product that allows zero as a value :</p>
<p><br /><span class="math display">$$
h_{logarithmic} = \left( \sum_{i=1}^3 d^+_i(o) \right) \times \alpha_{logarithmic}^{-\sum_{i=1}^3 d^-_i(o)}
$$</span><br /></p>
<p>This has the advantage to be able to be more finely tune the way the heuristic behaves, we can make incoming edges more or less damaging to the utility of an action.</p>
<!--
or even reverse the direction of the function by chosing $0 < \alpha_{logarithmic} < 1$.

$$h_1(o) = {p.d^+(o) \times \alpha^{-p.d^-(o)}
       \over
       {\Delta^p.d^+(o) \times \alpha^{-\Delta^p.d^-(o)}}
}$$ FIXME There is no way to compare steps and operators
$$h_2(o) = {{p.d^+(o) + o.d^+}
       \over
       {\alpha^{p.d^-(o) + o.d^-} }
}$$
$$h_3(o) ={{p.d^+(o) + \Delta^p.d^+(o) }
       \over
       { \alpha^{p.d^-(o) + \Delta^p.d^-(o)}}
}$$
-->
<h3 id="scalar-product">Scalar Product</h3>
<p>Another way to combine the data is to see it as vectors. A classical operation that can be done between two vectors is to get the dot product of the two. The formula become :</p>
<p><br /><span class="math display">$$
h_{scalar} = \alpha_{scalar} \times \left( d^+(o) \cdot -d^-(o) \right) = \alpha_{scalar} \sum_{i=1}^3 d^+_i(o) \times \left(-d^-_i(o) \right)
$$</span><br /></p>
<p>Since most of the time we get big negative values we chose to often make <span class="math inline">0 &lt; <em>α</em><sub><em>s</em><em>c</em><em>a</em><em>l</em><em>a</em><em>r</em></sub> &lt; 1</span>. <!-- Syntax coloring bug -_- --></p>
<h3 id="normative-vectorial-product">Normative Vectorial Product</h3>
<p>Another operation that can be done is to take the norm of the product of the two vector. This gives us :</p>
<p><br /><span class="math display"><em>h</em><sub><em>v</em><em>e</em><em>c</em><em>t</em><em>o</em><em>r</em><em>i</em><em>a</em><em>l</em></sub> = <em>α</em><sub><em>v</em><em>e</em><em>c</em><em>t</em><em>o</em><em>r</em><em>i</em><em>a</em><em>l</em></sub> × |<em>d</em><sup>+</sup>(<em>o</em>)×−<em>d</em><sup>−</sup>(<em>o</em>)|</span><br /> <!-- Too complex to develop--></p>
<!--
### Pearson's Product-Moment Coefficient

From the domain of statistics and correlation, we chose to experiment with Pearson product-moment correlation coefficient. It is a statistic that measures the linear correlation between two variables. FIXME I have absolutely no idea of why I chose that …

$$
h_{pearson} = \frac{n\sum d^+_i(o)\left(-d^-_i(o) \right)-\sum d^+_i(o)\sum -d^-_i(o)}
{\sqrt{n\sum d^+_i(o)^2-(\sum d^+_i(o))^2}~\sqrt{n\sum \left(-d^-_i(o) \right)^2-(\sum -d^-_i(o) )^2}}
$$  FIXME Whyyyyyyyyy -->
<!--
Degré simple : prod (d+ - d-)
Logarithmique : h1, h2, h3, etc
Produit scalaire : $sum(d+*d-)$
Norme Produit vectoriel : $Sqrt[Abs[-(y x') + x y']^2 + Abs[z x' - x z']^2 + Abs[-(z y') + y z']^2]$
Pearson's (hardman) product-moment coefficient :
$\frac{n\sum x_iy_i-\sum x_i\sum y_i}
{\sqrt{n\sum x_i^2-(\sum x_i)^2}~\sqrt{n\sum y_i^2-(\sum y_i)^2}}$
-->
<p>These heuristics have clearly different behaviours. To illustrate this we plotted the values given by each of them in our example in figure 3.</p>
<div class="figure">
<img src="graphics/utility.png" alt="Figure 3: Repesentation of the application of the different heuristics on the example. The solution is computed using a variation of the simple ration heuristic : \left (p.d^+(o) / {p.d^-(o)+1} \right) i(o) with i(o) being the number of instances of the operator in the solution plan p" />
<p class="caption">Figure 3: Repesentation of the application of the different heuristics on the example. The solution is computed using a variation of the <em>simple ration</em> heuristic : <span class="math inline">(<em>p</em>.<em>d</em><sup>+</sup>(<em>o</em>)/<em>p</em>.<em>d</em><sup>−</sup>(<em>o</em>)+1)<em>i</em>(<em>o</em>)</span> with <span class="math inline"><em>i</em>(<em>o</em>)</span> being the number of instances of the operator in the solution plan <span class="math inline"><em>p</em></span></p>
</div>
<p>We chose the best-suited alpha values in order to fit the solution.</p>
<p>What appears as the most appropriate heuristic is the scalar product. It gives good results on this example. However we see empirical results show that <strong>????</strong> <!--TODO actually clever analytic of this …--></p>
<h2 id="flaws-interference-and-priority">Flaws Interference and Priority</h2>
<p>Another problem lies in the selection of flaws. Indeed, the order that they are fixed can cause big problems. This kind of problems is called <strong>flaw interference</strong>.</p>
<p>These interferences include alternatives and cycles where there is no actual need to find an alternative to a causal link belonging to a closed walk. <!-- TODO IMPORTANT IMPLEMENTATION--> Another problem arise with the competition between orphans and subgoals: a subgoal might need an orphan and, therefore, remove the need for removal. Removing orphans before being sure that they won't be needed can be counter productive. The last interference is quite known in the domain. It is the interaction between subgoals and threats as subgoals can remove some threats without the need for intervention.</p>
<div class="figure">
<img src="graphics/interferences.png" alt="Figure 4: Schema of interferencial priorities and causal side effects" />
<p class="caption">Figure 4: Schema of interferencial priorities and causal side effects</p>
</div>
<!--FIXME where to put the schema, organisation or links ? 
Add the link between orphans and positive flaws and explain that being last considered it is not a side effect in practice-->
<p>All the possible interferences are listed in the figure 4. This schema shows the dirrection of the interference denoting the priotity of each flaws on the others. This leads us to the following ordered list of priority amongst flaws.</p>
<ol style="list-style-type: decimal">
<li><strong>Cycles</strong> that comes from the original domain proper plan are an indication that some operators are co-dependent and that is often where problems arise. This step is also the most susceptible to cause an early failure wich is very beneficial for the speed of the algorithm.</li>
<li><strong>Alternatives</strong> will cut causal links that have a better provider. It is necessary to do that first since they will add at least another subgoal to be fixed as a related flaw.</li>
<li><strong>Subgoals</strong> are the flaws that cause the most branching factor for POP algorithms. This is why we need to make sure that all open conditions are fixed before proceeding on finer refinements.</li>
<li><strong>Threats</strong> occurs quite often in the computation and requires lots of computation to check if they are applicable and are one of the most side-effect heavies. That is why we prioritise all related subgoals before threats because they can actually add causal links that will fix the threat without needing to do anything.</li>
<li><strong>Orphans</strong> are a fine optimisation of plans. They remove unneeded branches of the plan. However, these branches can be found out to be necessary for the plan in order to meet a subgoal. Since a branch can contain numerous actions it is preferable to let the orphan in the plan until we are sure that they won't be needed.</li>
</ol>
<h2 id="dynamic-flaw-selection">Dynamic Flaw Selection</h2>
<h1 id="sec:algorithm">LOLLIPOP Algorithm</h1>
<!--TODO redo all this plz-->
<p>With all these mechanisms defined we can now present the complete algorithm of LOLLIPOP :</p>
<div id="alg:lollipop" class="algorithm" name="All the yummy sweet lollipop">
<dt>Algorithm 3</dt>
<dd>
   
</dd>
</dl>
</div>
<p><!--TODO Need implementation first --></p>
<h2 id="domain-phase">Domain Phase</h2>
<p>The domain compilation phase is the first to quick in. It needs to parse and interpret a domain description input and make it usable by POP algorithms. This phase includes a cleaning phase that rules out illegal defects present in the initial description in order to be resilient to basic logic errors. <!--FIXME explains that more -->Most of that compilation include a translation from the expressive complexity of the input to a much simpler representation used in algorithms for performance. In this step, we also add our proper plan creation algorithm along with cycle detection on the result. This is used to cache cycle flaws in order to make the agenda population easier during runtime. We also keep a copy of the proper plan in memory and start ordering operators to allow fast goal selection.</p>
<h2 id="initialization">Initialization</h2>
<p>This phase comes as the problem is provided. It contains multiple steps.</p>
<ol style="list-style-type: decimal">
<li>Initial and Goal insertion into the proper plan to construct the initial partial plan.</li>
<li>Binding variables and inconsistencies detection.</li>
<li>Add the cached cycle flaws to the top of the agenda.</li>
<li>Search and add alternative flaws that aren't in cycles.</li>
<li>Search and add all obvious subgoals left unfulfilled in the plan.</li>
<li>Search for early threats in the partial plan and add them to the bottom of the agenda.</li>
<li>Search for all orphans and add them last to the agenda.</li>
</ol>
<h2 id="main-loop">Main Loop</h2>
<p>That part is similar to regular POP algorithms as it will almost only take flaws from the agenda and try to fix them one by one. On of the differences is that each time a flaw is fixed, it calls a special function that provides related flaws. In the case of cycles or alternative, we can have potential orphans and new subgoals that need to be fixed. In the case of subgoals, it can lead to new subgoals or threats. Threats are a bit special in that regard. They can't cause other flaws but are treated differently. The algorithm will always delay them, after all, current subgoals have been fixed. Then all orphans that are kept last may only cause other orphans <!-- what a sad world -->.</p>
<h2 id="properties-and-proofs">Properties and Proofs</h2>
<!--FIXME better title or something ? -->
<p><strong>????</strong> List of properties :</p>
<ul>
<li>Lollipop is complete and sound (which is quite important)</li>
<li>Lollipop will always output plans at least as good as POP (define a measure of quality first)</li>
<li>Lollipop won't need to compute more standard flaw than POP</li>
</ul>
<div class="proof" name="Proof of something">
<dt>Proof 1</dt>
<dd>
<p>The proof</p>
</dd>
</dl>
</div>
<h1 id="sec:results">Experimental Results</h1>
<!-- TODO add all the things-->
<p><strong>????</strong> Things we want to know :</p>
<ul>
<li>Is Lollipop faster than POP ? in which cases ?</li>
<li>Is the lollipop competitive in small problems ?</li>
<li>which heuristic are the best ? How to improve ? Meta heuristic ?</li>
<li>Measure the increase in quality</li>
<li>Plot the selection time and every other indicator taken in <span class="citation">(Kambhampati 1994)</span></li>
</ul>
<h1 id="conclusion" class="unnumbered">Conclusion</h1>
<!-- TODO
* Discussion of results and properties
* Summary of improvements
* Introducing soft solving and online planning.
* Online planning
* plan recognition and constrained planning-->
<h1 id="references" class="unnumbered">References</h1>
<div id="refs" class="references">
<div id="ref-blum_fast_1997">
<p>Blum, Avrim L., and Merrick L. Furst. 1997. “Fast Planning Through Planning Graph Analysis.” <em>Artificial Intelligence</em> 90 (1): 281–300.</p>
</div>
<div id="ref-coles_forwardchaining_2010">
<p>Coles, Amanda Jane, Andrew Coles, Maria Fox, and Derek Long. 2010. “Forward-Chaining Partial-Order Planning.” In <em>ICAPS</em>, 42–49.</p>
</div>
<div id="ref-gazen_combining_1997">
<p>Gazen, B. Cenk, and Craig A. Knoblock. 1997. “Combining the Expressivity of UCPOP with the Efficiency of Graphplan.” In <em>Recent Advances in AI Planning</em>, 221–33. Springer.</p>
</div>
<div id="ref-ghallab_automated_2004">
<p>Ghallab, Malik, Dana Nau, and Paolo Traverso. 2004. <em>Automated Planning: Theory &amp; Practice</em>. Elsevier.</p>
</div>
<div id="ref-gobelbecker_coming_2010">
<p>Göbelbecker, Moritz, Thomas Keller, Patrick Eyerich, Michael Brenner, and Bernhard Nebel. 2010. “Coming Up With Good Excuses: What to Do When No Plan Can Be Found.” In <em>ICAPS</em>, 81–88.</p>
</div>
<div id="ref-helmert_fast_2006">
<p>Helmert, Malte. 2006. “The Fast Downward Planning System.” <em>J. Artif. Intell. Res.(JAIR)</em> 26: 191–246.</p>
</div>
<div id="ref-hoffmann_ff_2001">
<p>Hoffmann, Jörg. 2001. “FF: The Fast-Forward Planning System.” <em>AI Magazine</em> 22 (3): 57.</p>
</div>
<div id="ref-kambhampati_design_1994">
<p>Kambhampati, Subbarao. 1994. “Design Tradeoffs in Partial Order (Plan Space) Planning.” In <em>AIPS</em>, 92–97.</p>
</div>
<div id="ref-kovacs_bnf_2011">
<p>Kovacs, Daniel L. 2011. “BNF Definition of PDDL 3.1.” <em>Unpublished Manuscript from the IPC-2011 Website</em>.</p>
</div>
<div id="ref-nguyen_reviving_2001">
<p>Nguyen, XuanLong, and Subbarao Kambhampati. 2001. “Reviving Partial Order Planning.” In <em>IJCAI</em>, 1:459–64.</p>
</div>
<div id="ref-penberthy_ucpop_1992">
<p>Penberthy, J Scott, Daniel S Weld, and others. 1992. “UCPOP: A Sound, Complete, Partial Order Planner for ADL.” <em>Kr</em> 92: 103–14.</p>
</div>
<div id="ref-peot_postponing_1994">
<p>Peot, Mark A., and David E. Smith. 1994. “Postponing Threats in Partial-Order Planning.”</p>
</div>
<div id="ref-richter_lama_2011">
<p>Richter, Silvia, Matthias Westphal, and Malte Helmert. 2011. “LAMA 2008 and 2011.” In <em>International Planning Competition</em>, 117–24.</p>
</div>
<div id="ref-sapena_combining_2014">
<p>Sapena, Oscar, Eva Onaindia, and Alejandro Torreno. 2014. “Combining Heuristics to Accelerate Forward Partial-Order Planning.” <em>Constraint Satisfaction Techniques for Planning and Scheduling</em>, 25.</p>
</div>
<div id="ref-w3c_rdf_2014">
<p>W3C. 2014. “RDF 1.1 Turtle: Terse RDF Triple Language.” <a href="https://www.w3.org/TR/turtle/" class="uri">https://www.w3.org/TR/turtle/</a>.</p>
</div>
<div id="ref-younes_vhpop_2003">
<p>Younes, H<span class="math inline">∖</span>a akan LS, and Reid G. Simmons. 2003. “VHPOP: Versatile Heuristic Partial Order Planner.” <em>Journal of Artificial Intelligence Research</em>, 405–30.</p>
</div>
</div>
